\section{Preservation}
Currently, more and more information is produced in digital form and more and more information has only a digital copy. This has enourmous implications for national and state archives, libraries, scientific institutions and business enterprieses but also the small companies and even private people as they often face data corruption and access problems in the long-term.
In general, digital preservation (or DP for short) copes with two main problems; preserving content (bit streams) for longer periods of time and ensuring these contents are accessible and understandable in the future. When talking about ``the future'' or ``longer periods of time'' we mean ``as long the content is needed''.
Of course this view is over simplified and there are many more challenges in digital preservation. From the fundamental technical problems through organizational and social challenges to practical and financial ones.

A good example to picture the problem and challenges in DP is presented in \cite{Lorie:2001:LTP:379437.379726} and in \cite{Rauber:2009:dpchallenges}. Imagine a file created today on a specific physical machine. This file is nothing more than a series of bits shaped in a specific format. In order to access this file in the long term, not only the bits and bytes have to be preserved but also the way of interpreting them (the format specification). This would also require to preserve the programms that can open, render and manipulate the file, which in turn will require the preservation of the dependency libraries and software packages as well as the operating system and the whole environment in which these programms or programm versions run. Failing to preserve only one single part of this chain and the file would be lost (even if the physical bit stream is still in tact).

An overview of the EU DP projects and activities is presented in this report \cite{strodl:2011:dpreport}. Starting in the mid nineties scientists started to recognize that these problems could lead to disasters and thus the need of digital preservation and its importance. By the beginning of the new millenium there were the first initiatives and projects in the EU that started focusing on research topics related to DP aiming the establishment of a community, identification of target groups and transfer of expertise (ERPANET, DELOS, DPE). The first scientific research was focused on topics such as standards, system concepts, selection and appraisal policies and fromat identification. Afterwards more technical and practical approaches were undertaken to research the preservation of simple digital objects such as office documents and images (PLANETS, CASPAR). All this helped the establishment of a solid community and a body of expertise.

Present initiatives include more fundamental research that tries to focus on more complex and interactive objects than simple and documents and data structures. Projects such as LiWA attempt to solve issues related to Web Archiving whereas projects such as TIMBUS and WF4Ever focus on the preservation of business processes and scientific workflows.
Other projects such as SCAPE build upon the solid framework established in the past and aim to improve the state of the art of DP by developning infrastructure and tools for scalable preservation actions and integrating them with automated policy based preservation planning and preservation watch systems and workflows.
%what does the future hold
\subsection{Most Common Approaches}
% the tools at hand, why are these important, trade offs
Through the years many tools and procedures were developed in order to preserve digital content. In the literature there are often different names for the same concepts. Here we present the most prominent ones. \newline

\textbf{Bit-Stream Preservation}
is the concept of copying the bits to a different medium with a different (physical) location. There are many different media, which can store digital data. Some are more stable than others, some are more popular than others. No matter on what type of medium you choose to store your data, CDs, DVDs, HardDrives, etc. it is not guaranteed that the data stream is safe. Through physical damage, bit rot or other disasters, there is a high chance that your digital storage media will fail to reproduce your bit stream. Thus on this lower level your only option would be to copy the streams to a different medium from time to time. This is strategy is often referred to as \textit{refreshing} \cite{Lee:2002:SOTADP}.

However, refreshing the data does not guarantee that it will be accessible in a later point in time as new media are also error prone. Therefore, approaches like LOCKSS (Lots Of Copies Keep Stuff Safe) make use of the distribution of many independent copies. Developed at the Stanford University the LOCKSS approach was implemented in a librarian software system that deploys many low cost copies of persistent web-caches and enables the detection and repairment of damages based on voting in opinion polls \cite{reich2001lpw, Maniatis:2003:PPR:1165389.945451}.
%eventually say other projects that use LOCKSS (ExLibris, JISC, Hoppla, etc.)
Following a LOCKSS approach, however, only minimizes the risk of losing data. If there is no effort spent in management of the copies, then it is fairly easy to lose track of the copies. For a software tool this might seem irrelevant but for a private user this is a real issue. Furthermore, even if enough well-managed copies were stored and the data stream was preserved, there is always the issue of software obscolescense and thus failure in the access and interpretation of the stream. \newline

\textbf{Logical Preservation} tries to cope exactly with this problem. In order to preserve not only the bit stream but also to ensure that the integrity of a digital object and that it can be interpreted in the long-term a conversion or migration approach is used \cite{Lee:2002:SOTADP}. New operating systems, new software tools or new versions are sometimes incompatible or unable to render and manipulate older formats. To cope with technology changes digital preservation often uses a conversion strategy where the data is migrated to another format, that is usually considered to be more stable. Usually a format is considered worthy and stable for preservation purposes, when it is standardized, the format specification is open and well-documented, there are no patent owners and license fees that apply. The Florida Center for Library Automation for example offers a report\footnote{http://fclaweb.fcla.edu/uploads/recFormats.pdf} with the recommended data formats for preservation purposes that is considered as a good reference and starting point. However, there is no ultimate reference table or no ultimate file format that fits all preservation purposes. From use case to use case different aspects have to be considered. Neither standards, nor migration tools alone can ensure that a digital document remains accessible and its integrity remains unharmed. Rothenberg gives a good overview of common flaws of these concepts in \cite{rothenberg:1999:ensuring} and summarizes important aspects that should be considered in DP.

Nonetheless, logical preservation is often applied within digital preservation systems and repositories. As it also has potential pitfalls, it is not to be taken lightly.
Another more practical downside to migration are the storage costs. Often the target format has a bigger footprint than the original. Also the conversion of huge amounts of data is an error prone process that is not easy to validate \cite{Lorie:2001:LTP:379437.379726}. Furthermore, if the migration path consists of several steps one have to make sure that all required meta data of the original is also migrated to the new versions of the objects. Another related issue is also quality assurance. As it is infeasible to check manually if the conversion process was successful there are very specific requirements and processes that have to be followed in order to automate the verification of the preservation action \cite{feng:2010:qrofm}.
All these and other issues have to be carefully taken into account before choosing such a preservation action.
\newline

\textbf{Emulation} has the verb "emulate" in its root, which means to immitate or reproduce.
In software terms an emulator is a software tool that immitates the behaviour of a (hardware) system/framework (usually an older one) in order to run other (obscolete) tools that are meant to run on the emulated system. Obviously, this approach offers can come in handy in regard of DP.
In \cite{rothenberg:1999:ensuring} Rothenberg gives an overview of a process for preservation that is based on an emulation process. The author states that not only the data (bit-stream) has to be stored but also the bit stream of the original program, the operating system and all other necessary parts, e.g. dependencies and used libraries. Also a thorough and complete description and specification of the underlying architecture have to be provided in a form that is readable by potential future emulator authors. If these prerequisites are met, an emulator that mimics the specific hardware needed to run the tool can be created.
Lorie points out in \cite{Lorie:2001:LTP:379437.379726} that the specification of the architecture has to be perfect and complete, which is a immensely difficult task. Another very important argument he makes is the evaluation of such an emulator. Even if all needed input exists and a hypothetical emulator was created, how can its correctnes as no original hardware device exists.

These and other reasons combined form one of the biggest downsides of emulation; cost. The effort, manpower and infrastructure needed to emulate an environment that renders digital documents is often more expensive than the value of the content of the documents themselves.

Nevertheless, emulation is widely used in specific branches, such as video gaming.
Guttenbrunner et al have evaluated different strategies for the preservation of console video games in \cite{guttenbrunner:2008:evaluating} and came to the conclusion that while migration shows very good results for the preservation of visual and audio components it completely fails in interactivity. Emulation on the other hand showed promissing results. All of these however, were strongly dependent on the sample objects that were emulated.

\subsection{Preservation Planning}
%what it is
%what is missing - CP
	%why does pp not work without cp
	%try to invert the big picture and state that cp is the actual thing and pp just does not work 	without it
\section{Analyzing Collections}
% what is important here
% meta data, distributions, aggregations, scalability, etc.

\section{Tools}
% what is present.
% explain that the tools for different types of contents are there, but no tool does
% collection profiling
% scape evaluation of characterisation...

\section{Quality Assurance Gap}
% despite the tools there is a big gap
% tools extract some measures, but there is almost no way, to assure that
% a measurement is correct

\section{Observations}
% summarize, what is going on,
% what is missing
% what is the problem of the current state of the art
% based on the research you made.