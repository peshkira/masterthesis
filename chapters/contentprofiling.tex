This chapter gives a good theoretical overview of the issue of content profiling for digital preservation. It summarises the goals and requirements of the process. Then a detailed definition of the preservation planning process is given followed by the relation of content profiling to digital preservation and how it fits in the bigger picture. The chapter defines the content profiling process and its necessary steps as well as gives insight into the importance and theory of representative sets. At the end the topic of continuous profiling and its benefit to preservation systems and experts is discussed shortly.

\section {Goals}
\label{sec:goals}
Content Profiling is a topic that combines two important parts of digital preservation and acts as the glue-ware, that holds up a digital preservation system in terms of its integrity.

On the one side, there are lower level technical processes, such as characterisation, which play an important role not only to preservation planning but to DP in general. These processes have to deal with single objects (even though the scale can be very large). As output they provide information to quality assurance activities and workflows as well as preservation planning. The identity and meta data extracted out of every single digital object is the scaffold that enables many other processes and applications to do their work. Consequently, the tools that provide this data care the responsibility for any subsequent process that relies on this data and its validity.

On the other side, there is preservation planning, which is a higher level process that deals with sets of objects. As such, preservation planning deals with a great deal of data, but is responsible that every single object complies to the requirements of a preservation expert after a preservation action is conducted. The usual size of real world-scenario collection (or set of objects) does not allow the test and inspection of every single object before and after each evaluated preservation action. Thus, a bigger picture of the content at hand plays an immense role in the choosing of representative samples and implicitly on the decision made by a preservation expert.

In order to ensure that these two different important parts of Digital Preservation fit together and provide a valid and effective outcome, an adaptor is needed. It has to transform the fine granular output of one process into an aggregated higher level input to the other.

Identification and characterisation are technical processes that can be conducted in automated fashion. Preservation Planning, on the other hand, is a process which can be automated only to a certain extent and will always rely on a human decision. Nonetheless, the degree of automation can be highly improved as the current state of the art is. 

A huge problem lies in the fact, that meta data extracting tool often provide data, that is not necessarily valid. The sparsity of the data presents another difficult task when evaluating content during preservation analysis and makes it even more difficult to grasp the peculiarities of the content. The volumes of the content and even of the meta data are high enough to present scalability challenges and sometimes even to dub the processing infeasible.

All these issues outline the goals of the content profiling process, which are summarised as follows:

\begin{itemize}
\item Enable automatic and scalable aggregation of sparse meta data provided by identification and characterisation tools.
\item Create and expose a well-defined, machine readable footprint of the content at hand, for other actors to use.
\item Enable planning experts to analyse the content. This includes:
 \begin{itemize}
  \item obtaining an overview of the content types and formats, of the collection, but is not restricted only to these.
  \item generating statistical reports about the size of the content.
  \item filtering the content into homogeneous sub sets, based on multiple characteristics.
 \end{itemize}
\item Export the raw (sparse) meta data in a common format that can be processed by other analysis software.
\item Select representative subsets based on different approaches, that make sense in different use cases.
\item Browse the raw meta data of objects.
\end{itemize}
% goals

\section{Preservation Planning}
The preservation planning process is a well-defined workflow consisting of three phases with several steps amounting to 11 altogether \cite{STR07_jcdl}. The process specification was created during the PLANETS project and has been verified by numerous case studies since then. 

In the first phase, \textit{``Define Requirements''}, the scope of the preservation plan is demarcated. The preservation expert has to follow three steps; to provide information about the collection, environment etc. (\textbf{Define Basis}), to choose representative sample records for experimentation (\textbf{Define Sample Records}) and to identify the requirements for the preservation plan or the so called objective tree, which summarised high-level goals of the plan (\textbf{Identify Requirements}). 

In the second phase, \textit{``Evaluate Alternatives''}, another 5 steps have to be followed. Starting with the definition of alternatives (\textbf{Define Alternatives}), the responsible preservation expert has to choose a set of potential actions, with all related informations, such as environment, tool invocation parameters, etc. In the following (\textbf{GO/NO-GO}) step a decisions is made whether to proceed or not based on each preservation action, the estimated resources and the defined requirements. After that the planner has to create suitable experiments (\textbf{Develop Experiment}), which are well-documented, repeatable set of actions with their environment and the capability to capture their results. In the following (\textbf{Run Experiment}) step, each preservation action is executed against the chosen sample records in order to obtain different results. In the last step of this phase (\textbf{Evaluate Results}) the results of the experiment output is evaluated against the objective tree in order to check if the identified requirements were met or not.

The third phase, \textit{``Consider Results''}, is responsible for the objective analysis of the results. In its first step (\textbf{Transformed Measured Value}) all experiment results are transformed into the same scale (0-5) making use of special transformation tables and utility function. The following (\textbf{Set Importance Factor}) step provides the ability to equal the weight of different parts of the requirement objective tree as not all goals are equally important. In the last step (\textbf{Analyse Results}) all measures are aggregated per objective and provide the planner with a preservation action recommendation and the necessary basis for a decision.

\begin{figure}[tbp]
\begin{center}
\includegraphics[width=5in]{figures/contentprofiling/planningworkflow.png}
\caption{Overview of PLANETS Preservation Planning workflow \cite{Becker:2008:PSO:1378889.1378954}.}
\label{fig:planningworkflow}
\end{center}
\end{figure}

The workflow is depicted in figure \ref{fig:planningworkflow} and provides the current state of the art in preservation planning matters. Although it provides a very solid theoretical ground and a complete specification that is working in practice, there is one flaw in the concept, which leaves space for huge errors caused by human incompetence, lack of knowledge and understanding of the collection or even sloppiness.

As one can see from the workflow all the results strongly depend on the defined goals and the analysis of the experiments output. We assume that a preservation expert will understand the objectives of his organisation. Since the identification of requirements and the setting of the goals and objectives are very important steps that are also strongly dependent on the organisational background of the preservation expert, we also assume that it is unlikely they will cause errors and misunderstandings in later steps. Also if the planner happens to choose wrong preservation action alternatives, the result will be in the worst case scenario a 'Do Nothing' alternative, which will not solve the problem at hand, but will also not do any damages.
However, there is one step that could have serious implications and even cause damage or resource loss if taken lightly. Consider the following example, where the chosen sample records are picked up at random from a medium sized collection with several thousands of objects. Then the experiments show a particular preservation action is very feasible and the planner chooses to execute this action over the whole content, due to the experiments output and the consequent analysis. Although the analysis, and the decision are perfectly valid (in their implementation and execution), it could turn out that the result of the preservation operation does not meet the requirements defined. This could happen, due to many different aspects in the format and content profile of the collection at hand. To summarise, the defined requirements and objectives are ok, the experiments are valid, the analysis is correct, but the overall results are not feasible due to the false premise, that the random chosen set of sample records is representative. Damage could be done, if there is no reasonable and thorough quality assurance process afterwards. This work addresses this problem and tries to prevent it by reducing the bias of the experiments. This ought to be achieved by thorough analysis and automatic representative selection in the early steps of the workflow.

One can argue, that no real preservation expert will choose representatives at random. Although this might be true, there are numerous other factors that have to be considered when choosing the representatives and since the collections that are worth preserving are often big enough, the overhead for the preservation expert is just not feasible to select them by hand. Thus the representatives are usually chosen by format and format version in combination with their size (minimum, maximum and average).

Note, that there is a fourth phase (\textit{``Build preservation plan``}) which adds an important part to the workflow and results in an applicable real world preservation plan artefact. The planning tool PLATO, developed at the University of Technology in Vienna, implements this process and appends a fourth phase, where the user/planning expert can create an executable plan, which can be deployed within a repository. The preservation action plan is a well defined specification that serves the purpose of documentation of the decision and contains the executable part, which specifies the tools, environment and parameters to use during the preservation operation.

However, in its current release, the planning tool supports only manual sample records definition. Although it assists the planner with integrated characterisation tools, it cannot provide higher certainty in the validity of the chosen representatives. Integration with another tool that provides a complete content profile (generated in an automatic fashion) would provide a huge benefit to preservation planners.

\section{Profiling}
\label{sec:content_profiling}
This section describes content profiling and discusses its prerequisites and requirements. Afterwards it proposes an approach of creating a profile in an automatic fashion. Afterwards the enhancement of preservation planning through automation support during analysis of content in real world DP scenarios and integration with other DP information systems is discussed.

\subsection{Prerequisites}
In order to generate a good and valid content profile some requirements have to be met. For one, the characterisation has to be provided, but also its data quality in terms of validity, normalisation, etc. has to be guaranteed.

\subsubsection{Characterisation Data}
Clearly, to profile a set of digital objects, the meta data of the set of objects is needed. There are arguments whether or not the characterisation process should be part of profiling. We support the opinion, that characterisation should be done before and its output serves as input to a profiler. There are two main reasons for this. One, in real world scenarios content is usually stored in special archives (digital repositories), which extract the meta data out of the digital objects upon ingest in these systems. Thus, it does not make sense to access the original objects again and run a potentially time-consuming process over them again. The second reason is that profiling is an analysis step and should not make use of one characterisation tool, but should try to be agnostic to the meta data format. After all meta data are just key-value pairs. It is much better to support different formats and transform them to an internal model instead of restricting the whole process to one format.

\subsubsection{Data \& Normalisation}
In order to achieve the goals outlined above, the meta data provided by the characterisation tools has to be normalised and should fit into a unified model no matter its origin.

Consider the following example. Two characterisation tools provide meta data for document formats and measure different characteristics for these documents but one - the number of embedded tables in the document. Only, the first tool reports this characteristic under the name 'tableCount' and the second 'nrTables'. Semantically both measures provide the same information and it will be very valuable to a planner to know if both tools provided the same result or not. Also the coverage of this property within the characterised set will be potentially higher, since characterisation tools do not often manage to extract every property out of every digital object.

This example reveals two problems. Firstly, if the data is not normalised at all, the sparsity will be even higher and this will compromise the analysis and the gained knowledge about the collection. The second problem is the difficulty of normalisation itself. In the given example, it is fairly easy to assume that the semantics of the given property are the same. However, this is not true for all characteristics of all properties. Correct normalisation requires the domain knowledge of DP experts, the developers of the characterisation tool and potentially other experts. If it is done, wrong the whole result of a profile can be compromised.

Because of these problems, it is very important that characterisation tools provide valid and well-documented data. On the other hand, profiling tools, should allow the normalisation of such data, if the characteristics are well-defined and it is clear that the semantics of two or more properties are the same.

For this purpose, a simple but flexible domain model has to be created that allows, properties, their measurements, provenance information and more to be stored at one place. The data structure has to enable efficient aggregation and querying of the data. Important aspects that have to be taken into consideration during design are the sparsity of the meta data, the different data types of the measurements. As discussed, the validity of measurements is often unreliable, so it is important to keep the provenance information of the measurements. If the profiler is ought to keep track of continuous data, then the time of measurement should also be taken into account. Another important issue is the identifier of each object, which has to be able to point to the original in the repository system.

%Another important aspect of the data is its validity. Obviously if the data that is provided is valid and of high quality, then the created profiles and the chosen representative sample objects will solely rely on the processes and algorithms involved. If these processes and algorithms are validated and on their terms of high quality, then the result will be a profile that enables unbiased experiments in preservation planning activities. As profiling depends on the meta data and the tools that are able to extract it, it is of great importance that quality assurance is conducted. Unfortunately, the state of the art does not provide many methods of checking the validity of a characterisation tool. %cite? qa in characterization?
% why is it important (iteration of before)


\subsection{The process}
Content profiling consists of three main parts; data harvesting, data aggregation and analysis.

The first part is responsible for gathering and processing the output of the characterisation processes in a DP system and adapting it to the internal model of the profile framework. Since DP scenarios usually make use of digital repositories, it should be possible to harvest the meta data directly from the repository. This can present a potential issue, due to the differences in the interfaces of repositories. Nonetheless, it has to be considered within the design of the system. Of course the approach could include characterisation as part of the process, which would result in a collection of meta data files. In such a case, the disadvantages discussed above, should be considered.

After parsing the meta data, special post-processing steps can be applied to each meta data object in order to refine data gathering. For example, if the tool does not provide normalised data, special actions can be undertaken to deal with this issue. Also, if there are conflicted values of the same property provided by different meta data sources, data cleanup actions and rules can be executed. These post-processing steps can have huge impact on the final result and thus should not be executed lightly, but only through special configuration steps, done by preservation experts, that understand the source of the meta data, the operations of the identification and characterisation tools and the implications of such alterations of the data. Thus, such functionality of the content profiler has to be done in a flexible fashion and has to allow special configuration that can alter and turn on and off such behaviour. Once the data fits the internal representation of the profiler, it can be stored in a data base per digital object level and thus allow further processing by the profiling process.

The next step of aggregation can be done either after the data is parsed, normalised and stored or on demand if the underlying data store provides the necessary facilities. It is responsible to present the big picture of the data in a smaller footprint and structured form, so that other programs and tools can integrate with it. It should provide enough flexibility to understand the data but also a smaller footprint. Here different queries and aggregations can be executed, stored or cached in order to enable analysis or export of the data. An important issue that has to be considered is how data is going to be updated during time and content changes. 

The last step is a higher level service on top of the framework, that has to be conducted mostly manually by the preservation expert. This means that it will rely on specific decisions and will need the input of a user. Nonetheless, it should be automated as much as possible and shall support the user in her decisions. This includes querying the data and gaining knowledge about the content by drilling it down based on different characteristics.

\subsection{Output}
% specification (representation, xml, rdf)
The profiler has to present the aggregated data in form that is usable to planning experts, but also allow the export of the data into other formats for further processing. Furthermore, the preservation process will be enhanced, only if the data
can be obtained in a machine readable format, so that other tools can interact with it.

There are numerous ways of representing such a profile. However, from the observations of the previous chapter there are number of goals that it has to fulfil. It has to follow a well-defined schema; the profile has to be in a structured, machine readable form, as it will act as input to other information systems in the DP landscape.

Clearly, it has to aggregate the raw data, so that it has a small enough footprint, but still provide enough insight into the content at hand. The profile should be able to separate the content based on different characteristics. Here it is important that a user or a user application is able to go one step back and examine the filtering criteria. Finally, the profile should include a small set of representative objects, with their full characteristics and a way to identify all objects in a set or collection that fit into the specified filter. The latter could be done, either via some kind of query or just by a list of objects outlines the identifiers of all selected objects.

%Appendix A shows a proposal of the first format schema.

\subsection{Integration with DP Systems}
% integration with other tools, such as repositories and planning tools
A content profiler tool can provide benefit to numerous systems and users in a Digital Preservation environment. On the one hand, it can act as direct input to Preservation Planning and can spare a planning expert a lot of time defining and outlining a plan. 

On the other hand, it supplies the needed facilities to analyse new content ingested within a repository and can act as input to monitoring systems, that can raise alerts if certain conditions are violated. For example, an organisation should have a policy that defines, which objects have to be preserved and one that defines how many objects of a certain type can exist without a valid plan associated to them. Now consider that each day a great deal of new objects is ingested into the repository of that particular organisation. By the end of a certain time period a content profile is regenerated and a monitoring system obtains and reevaluates this result against the organisational policies. It will be fairly easy to detect a potential threat and to notify the involved stakeholders and actors in order to resolve it, provided all of these pieces of the puzzle fit together.

Another great deal of the combination of such a content profile and monitoring system can be a global content profile. If enough organisations, such as Web Archives, Libraries, etc. are willing to share the profiles (or parts of them) they have with such a central monitor, the result will most probably be a representative overview of the global state of the world \cite{duretec:2012:watch}. Valuable information, such as the number of formats used within a certain domain, the preferred type of files within a user community, the emergence of new file formats, the decline of old file formats and much more can be detected. Provided that enough content-holding institutions are ready to sacrifice a bit of their privacy, the mutual benefit could be something much more valuable.

\subsection{Limitations \& Pitfalls}
There are a number of problems and potential pitfalls, that have to be considered when designing and implementing the proposed framework.

A very important issue is the removal of objects. It is, unusual that objects are removed from archives, but it is not impossible (e.g. after migration). Once an object (or a set of objects) is removed, the profile immediately becomes invalid and should be regenerated. If the source system (repository, file system, etc) does not provide information of which objects were deleted, the whole process should be repeated from the beginning. This could be time and resource consuming.

%TODO add more

\section{Representative Sets}
\label{sec:representative_sets}
% what is the problem,
% how do we create them/find them
TBD

\section{Continuos Profiling}
TBD
