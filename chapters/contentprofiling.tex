This chapter gives a good theoretical overview of the issue of content profiling for digital preservation. It summarises the goals and requirements of the process. Then a detailed definition of the preservation planning process is given followed by the relation of content profiling to digital preservation and how it fits in the bigger picture. The chapter defines the content profiling process and its necessary steps as well as gives insight into the importance and theory of representative sets. At the end the topic of continuous profiling and its benefit to preservation systems and experts is discussed shortly.

\section {Goals}
\label{sec:goals}
Content Profiling is a topic that combines two important parts of digital preservation and acts as the glue-ware, that holds up a digital preservation system in terms of its integrity.

On the one side, there are lower level technical processes, such as characterisation, which play an important role not only to preservation planning but to DP in general. These processes have to deal with single objects (even though the scale can be very large). As output they provide information to quality assurance activities and workflows as well as preservation planning. The identity and meta data extracted out of every single digital object is the scaffold that enables many other processes and applications to do their work. Consequently, the tools that provide this data care the responsibility for any subsequent process that relies on this data and its validity.

On the other side, there is preservation planning, which is a higher level process that deals with sets of objects. As such, preservation planning deals with a great deal of data, but is responsible that every single object complies to the requirements of a preservation expert after a preservation action is conducted. The usual size of real world-scenario collection (or set of objects) does not allow the test and inspection of every single object before and after each evaluated preservation action. Thus, a bigger picture of the content at hand plays an immense role in the choosing of representative samples and implicitly on the decision made by a preservation expert.

In order to ensure that these two different important parts of Digital Preservation fit together and provide a valid and effective outcome, an adaptor is needed. It has to transform the fine granular output of one process into an aggregated higher level input to the other.

Identification and characterisation are technical processes that can be conducted in automated fashion. Preservation Planning, on the other hand, is a process which can be automated only to a certain extent and will always rely on a human decision. Nonetheless, the degree of automation can be highly improved as the current state of the art is. 

A huge problem lies in the fact, that meta data extracting tool often provide data, that is not necessarily valid. The sparsity of the data presents another difficult task when evaluating content during preservation analysis and makes it even more difficult to grasp the peculiarities of the content. The volumes of the content and even of the meta data are high enough to present scalability challenges and sometimes even to dub the processing infeasible.

All these issues outline the goals of the content profiling process, which are summarised as follows:

\begin{itemize}
\item Enable automatic and scalable aggregation of sparse meta data provided by identification and characterisation tools.
\item Create and expose a well-defined, machine readable footprint of the content at hand, for other actors to use. This has to summarise the following information.
 \begin{itemize}
  \item an identifier of the content.
  \item the characteristics of the content, such as mime types, formats, size, but also any other that is of interest to preservation planning.
  \item the sample records.
  \item the scope of the profile (e.g. the object identifiers of the collection, within a repository interface)
 \end{itemize}
\item Enable planning experts to analyse the content. This includes:
 \begin{itemize}
  \item obtaining an overview of the content types and formats, of the collection, but is not restricted only to these.
  \item generating statistical reports about the size of the content.
  \item filtering the content into homogeneous sub sets, based on multiple characteristics.
 \end{itemize}
\item Export the raw (sparse) meta data in a common format that can be processed by other analysis software.
\item Select representative subsets based on different approaches, that make sense in different use cases.
\item Browse the raw meta data of objects.
\end{itemize}
% goals

\section{Preservation Planning}
The preservation planning process is a well-defined workflow consisting of three phases with several steps amounting to 11 altogether \cite{STR07_jcdl}. The process specification was created during the PLANETS project and has been verified by numerous case studies since then. 

In the first phase, \textit{``Define Requirements''}, the scope of the preservation plan is demarcated. The preservation expert has to follow three steps; to provide information about the collection, environment etc. (\textbf{Define Basis}), to choose representative sample records for experimentation (\textbf{Define Sample Records}) and to identify the requirements for the preservation plan or the so called objective tree, which summarised high-level goals of the plan (\textbf{Identify Requirements}). 

In the second phase, \textit{``Evaluate Alternatives''}, another 5 steps have to be followed. Starting with the definition of alternatives (\textbf{Define Alternatives}), the responsible preservation expert has to choose a set of potential actions, with all related informations, such as environment, tool invocation parameters, etc. In the following (\textbf{GO/NO-GO}) step a decisions is made whether to proceed or not based on each preservation action, the estimated resources and the defined requirements. After that the planner has to create suitable experiments (\textbf{Develop Experiment}), which are well-documented, repeatable set of actions with their environment and the capability to capture their results. In the following (\textbf{Run Experiment}) step, each preservation action is executed against the chosen sample records in order to obtain different results. In the last step of this phase (\textbf{Evaluate Results}) the results of the experiment output is evaluated against the objective tree in order to check if the identified requirements were met or not.

The third phase, \textit{``Consider Results''}, is responsible for the objective analysis of the results. In its first step (\textbf{Transformed Measured Value}) all experiment results are transformed into the same scale (0-5) making use of special transformation tables and utility function. The following (\textbf{Set Importance Factor}) step provides the ability to equal the weight of different parts of the requirement objective tree as not all goals are equally important. In the last step (\textbf{Analyse Results}) all measures are aggregated per objective and provide the planner with a preservation action recommendation and the necessary basis for a decision.

\begin{figure}[tbp]
\begin{center}
\includegraphics[width=5in]{figures/contentprofiling/planningworkflow.png}
\caption{Overview of PLANETS Preservation Planning workflow \cite{Becker:2008:PSO:1378889.1378954}.}
\label{fig:planningworkflow}
\end{center}
\end{figure}

The workflow is depicted in figure \ref{fig:planningworkflow} and provides the current state of the art in preservation planning matters. Although it provides a very solid theoretical ground and a complete specification that is working in practice, there is one flaw in the concept, which leaves space for huge errors caused by human incompetence, lack of knowledge and understanding of the collection or even sloppiness.

As one can see from the workflow all the results strongly depend on the defined goals and the analysis of the experiments output. We assume that a preservation expert will understand the objectives of his organisation. Since the identification of requirements and the setting of the goals and objectives are very important steps that are also strongly dependent on the organisational background of the preservation expert, we also assume that it is unlikely they will cause errors and misunderstandings in later steps. Also if the planner happens to choose wrong preservation action alternatives, the result will be in the worst case scenario a 'Do Nothing' alternative, which will not solve the problem at hand, but will also not do any damages.
However, there is one step that could have serious implications and even cause damage or resource loss if taken lightly. Consider the following example, where the chosen sample records are picked up at random from a medium sized collection with several thousands of objects. Then the experiments show a particular preservation action is very feasible and the planner chooses to execute this action over the whole content, due to the experiments output and the consequent analysis. Although the analysis, and the decision are perfectly valid (in their implementation and execution), it could turn out that the result of the preservation operation does not meet the requirements defined. This could happen, due to many different aspects in the format and content profile of the collection at hand. To summarise, the defined requirements and objectives are ok, the experiments are valid, the analysis is correct, but the overall results are not feasible due to the false premise, that the random chosen set of sample records is representative. Damage could be done, if there is no reasonable and thorough quality assurance process afterwards. This work addresses this problem and tries to prevent it by reducing the bias of the experiments. This ought to be achieved by thorough analysis and automatic representative selection in the early steps of the workflow.

One can argue, that no real preservation expert will choose representatives at random. Although this might be true, there are numerous other factors that have to be considered when choosing the representatives and since the collections that are worth preserving are often big enough, the overhead for the preservation expert is just not feasible to select them by hand. Thus the representatives are usually chosen by format and format version in combination with their size (minimum, maximum and average).

Note, that there is a fourth phase (\textit{``Build preservation plan``}) which adds an important part to the workflow and results in an applicable real world preservation plan artefact. The planning tool PLATO, developed at the University of Technology in Vienna, implements this process and appends a fourth phase, where the user/planning expert can create an executable plan, which can be deployed within a repository. The preservation action plan is a well defined specification that serves the purpose of documentation of the decision and contains the executable part, which specifies the tools, environment and parameters to use during the preservation operation.

However, in its current release, the planning tool supports only manual sample records definition. Although it assists the planner with integrated characterisation tools, it cannot provide higher certainty in the validity of the chosen representatives. Integration with another tool that provides a complete content profile (generated in an automatic fashion) would provide a huge benefit to preservation planners.

\section{Profiling}
\label{sec:content_profiling}
This section describes content profiling and discusses its prerequisites and requirements. Afterwards it proposes an approach of creating a profile in an automatic fashion. Afterwards the enhancement of preservation planning through automation support during analysis of content in real world DP scenarios and integration with other DP information systems is discussed.

\subsection{Prerequisites}
In order to generate a good and valid content profile some requirements have to be met. For one, the characterisation has to be provided, but also its data quality in terms of validity, normalisation, etc. has to be guaranteed.

\subsubsection{Characterisation Data}
Clearly, to profile a set of digital objects, the meta data of the set of objects is needed. There are arguments whether or not the characterisation process should be part of profiling. We support the opinion, that characterisation should be done before and its output serves as input to a profiler. There are two main reasons for this. One, in real world scenarios content is usually stored in special archives (digital repositories), which extract the meta data out of the digital objects upon ingest in these systems. Thus, it does not make sense to access the original objects again and run a potentially time-consuming process over them again. The second reason is that profiling is an analysis step and should not make use of one characterisation tool, but should try to be agnostic to the meta data format. After all meta data are just key-value pairs. It is much better to support different formats and transform them to an internal model instead of restricting the whole process to one format.

\subsubsection{Data \& Normalisation}
In order to achieve the goals outlined above, the meta data provided by the characterisation tools has to be normalised and should fit into a unified model no matter its origin.

Consider the following example. Two characterisation tools provide meta data for document formats and measure different characteristics for these documents but one - the number of embedded tables in the document. Only, the first tool reports this characteristic under the name 'tableCount' and the second 'nrTables'. Semantically both measures provide the same information and it will be very valuable to a planner to know if both tools provided the same result or not. Also the coverage of this property within the characterised set will be potentially higher, since characterisation tools do not often manage to extract every property out of every digital object.

This example reveals two problems. Firstly, if the data is not normalised at all, the sparsity will be even higher and this will compromise the analysis and the gained knowledge about the collection. The second problem is the difficulty of normalisation itself. In the given example, it is fairly easy to assume that the semantics of the given property are the same. However, this is not true for all characteristics of all properties. Correct normalisation requires the domain knowledge of DP experts, the developers of the characterisation tool and potentially other experts. If it is done, wrong the whole result of a profile can be compromised.

Because of these problems, it is very important that characterisation tools provide valid and well-documented data. On the other hand, profiling tools, should allow the normalisation of such data, if the characteristics are well-defined and it is clear that the semantics of two or more properties are the same.

For this purpose, a simple but flexible domain model has to be created that allows, properties, their measurements, provenance information and more to be stored at one place. The data structure has to enable efficient aggregation and querying of the data. Important aspects that have to be taken into consideration during design are the sparsity of the meta data, the different data types of the measurements. As discussed, the validity of measurements is often unreliable, so it is important to keep the provenance information of the measurements. If the profiler is ought to keep track of continuous data, then the time of measurement should also be taken into account. Another important issue is the identifier of each object, which has to be able to point to the original in the repository system.

%Another important aspect of the data is its validity. Obviously if the data that is provided is valid and of high quality, then the created profiles and the chosen representative sample objects will solely rely on the processes and algorithms involved. If these processes and algorithms are validated and on their terms of high quality, then the result will be a profile that enables unbiased experiments in preservation planning activities. As profiling depends on the meta data and the tools that are able to extract it, it is of great importance that quality assurance is conducted. Unfortunately, the state of the art does not provide many methods of checking the validity of a characterisation tool. %cite? qa in characterization?
% why is it important (iteration of before)

\subsection{The process}
Content profiling consists of three main parts; data harvesting, data aggregation and analysis.

The first part is responsible for gathering and processing the output of the characterisation processes in a DP system and adapting it to the internal model of the profile framework. Since DP scenarios usually make use of digital repositories, it should be possible to harvest the meta data directly from the repository. This can present a potential issue, due to the differences in the interfaces of repositories. Nonetheless, it has to be considered within the design of the system. Of course the approach could include characterisation as part of the process, which would result in a collection of meta data files. In such a case, the disadvantages discussed above, should be considered.

After parsing the meta data, special post-processing steps can be applied to each meta data object in order to refine data gathering. For example, if the tool does not provide normalised data, special actions can be undertaken to deal with this issue. Also, if there are conflicted values of the same property provided by different meta data sources, data cleanup actions and rules can be executed. These post-processing steps can have huge impact on the final result and thus should not be executed lightly, but only through special configuration steps, done by preservation experts, that understand the source of the meta data, the operations of the identification and characterisation tools and the implications of such alterations of the data. Thus, such functionality of the content profiler has to be done in a flexible fashion and has to allow special configuration that can alter and turn on and off such behaviour. Once the data fits the internal representation of the profiler, it can be stored in a data base per digital object level and thus allow further processing by the profiling process.

The next step of aggregation can be done either after the data is parsed, normalised and stored or on demand if the underlying data store provides the necessary facilities. It is responsible to present the big picture of the data in a smaller footprint and structured form, so that other programs and tools can integrate with it. It should provide enough flexibility to understand the data but also a smaller footprint. Here different queries and aggregations can be executed, stored or cached in order to enable analysis or export of the data. An important issue that has to be considered is how data is going to be updated during time and content changes. 

The last step is a higher level service on top of the framework, that has to be conducted mostly manually by the preservation expert. This means that it will rely on specific decisions and will need the input of a user. Nonetheless, it should be automated as much as possible and shall support the user in her decisions. This includes querying the data and gaining knowledge about the content by drilling it down based on different characteristics.

\subsection{Limitations \& Pitfalls}
There are a number of problems and potential pitfalls, that have to be considered when designing and implementing the proposed framework.

A very important issue is the removal of objects. It is, unusual that objects are removed from archives, but it is not impossible (e.g. after migration). Once an object (or a set of objects) is removed, the profile immediately becomes invalid and should be regenerated. If the source system (repository, file system, etc) does not provide information of which objects were deleted, the whole process should be repeated from the beginning. This could be time and resource consuming.

The same goes if new meta data is acquired and merged within the current profile. Here normalisation has to be kept intact and also any aggregation that might be influenced should be recalculated.

Clearly, the quality of a content profile is highly dependant on the quality of meta data.

If data normalisation is done via the framework and is not part of the meta data input, then very specific expertise will be required to do this task efficiently.

\subsection{Output}
% specification (representation, xml, rdf)
The profiler has to present the aggregated data in form that is usable to planning experts, but also allow the export of the data into other formats for further processing. Furthermore, the preservation process will be enhanced, only if the data
can be obtained in a machine readable format, so that other tools can interact with it.

There are numerous ways of representing such a profile. However, from the observations of the previous chapter there are number of goals that it has to fulfil. It has to follow a well-defined schema; the profile has to be in a structured, machine readable form, as it will act as input to other information systems in the DP landscape.

Clearly, it has to aggregate the raw data, so that it has a small enough footprint, but still provide enough insight into the content at hand. The profile should be able to separate the content based on different characteristics. Here it is important that a user or a user application is able to go one step back and examine the filtering criteria. Finally, the profile should include a small set of representative objects, with their full characteristics and a way to identify all objects in a set or collection that fit into the specified filter. The latter could be done, either via some kind of query or just by a list of objects outlines the identifiers of all selected objects.

Having such a content profile is the foundation for integration with other DP software systems, such as preservation watch and monitoring systems, simulation environments and planning components. For this integration some higher level features based on the exported profile and representative sets can be done.

%Appendix A shows a proposal of the first format schema.

\section{Continuos Profiling}
A content profiler tool can provide benefit to numerous systems and users in a Digital Preservation environment. On the one hand, it can act as direct input to Preservation Planning and can spare a planning expert a lot of time defining and outlining a plan. On the other hand, it supplies the needed foundation to do continuous profiling. Continuous profiling refers to two higher level applications of content profiling when integrating with other systems. The one is profiling data based on its creation or harvesting date. The other one is the continuous monitoring of a profile of a collection and all its changes through time. Here we give some overview of both ideas and summarise how these can be achieved and what benefits they will provide to planning experts.

\subsection{Monitoring}
As discussed in the previous chapter monitoring is an external phase of the preservation planning environment that provides feedback and can trigger reevaluation of a plan. If a content profile generated from the content of a repository is observed continuously by such a monitoring system, then certain triggers can be raised when conditions are violated.
For example, an organisation that has a policy that defines which objects have to be preserved and one that defines how many objects of a certain type can exist without a valid plan associated to them. Now consider that each day a great deal of new objects is ingested into the repository of that particular organisation. By the end of a certain time period a content profile is regenerated and a monitoring system obtains and reevaluates this result against the organisational policies. It will be fairly easy to detect a potential threat and to notify the involved stakeholders and actors in order to resolve it, provided all of these pieces of the puzzle fit together.

Another great deal of the combination of such a content profile and monitoring system can be a global content profile. If enough organisations, such as Web Archives, Libraries, etc. are willing to share the profiles (or parts of them) they have with such a central monitor, the result will most probably be a representative overview of the global state of the world \cite{becker-ipres2012}. Valuable information, such as the number of formats used within a certain domain, the preferred type of files within a user community, the emergence of new file formats, the decline of old file formats and much more can be detected. Provided that enough content-holding institutions are ready to sacrifice a bit of their privacy, the mutual benefit could be something much more valuable.

The integration between a content profiling tool and such a monitoring system presents some technical issues and questions that have to be answered, as content holding institutions make use of different repository and systems. Nonetheless, if there is a profiling tool running within such a system or near the data and conforms to a standardised machine-readable output, then such an integration is feasible. A design proposition of such a novel preservation watch system is discussed in \cite{duretec:2012:watch, FariaPDBFR12}.


%TODO
%describe minitoring
% how does it work in terms of content profiling
% need of an exposed service
% cite papers at the according places and may be not up there

\subsection{Simulation \& Trend Analysis}
Another interesting possibility for integration is with simulation software that tries to analyse the trends of different characteristics of the content over time. Among these could be future development of the format profiles, detection of format obsolescence and emerging formats, size fluctuations, or any other preservation related characteristic.

Weihs et.al demonstrate a simulation prototype \cite{TUW-206758} that can simulate the evolution of a repository over time. The input used for the simulations are different models and configurations of the content, as well as the potential result of preservation actions over this content. An integration between such a tool and a real world content profile could be an interesting approach to examine and validate how simulation can influence digital preservation decisions, but also provide new requirements and goals for content profiling  as well.

Other interesting aspects to simulate could be the format footprint of a repository over time. There are different theories regarding the lifetime of formats and a debate pro and contra for following a direct migration strategy or whether or not this should be left to the so called network effects of data sharing, which should prevent the damages of obsolescence \cite{Rosenthal:1January2010:0737-8831:195}. Jackson gives a good summary of this debate and asks the question: ''Where is the evidence?'' In order to find out, experiments of format usage over large digital corpora of the British Web Archive are conducted. The results are presented in \cite{journals/corr/abs-1210-1714} and suggest that there are indeed formats that live longer than five years, however there were number of formats fading from use and these should be studied closely. Apart from being very interesting, these (unstructured) profiles could act as input to simulation environments, where based on the past trends new hypothesis for the future development are tested and used for planning.

One example of such trend results is presented in figure \ref{fig:trends_html}. There the different html versions of about 1.5 million webarchive digital objects from the the danish web archive are presented through the years. The data for this graph was processed with the help of the backend of a content profile tool following the suggested approach in this chapter. It was produced for another thesis conducted at the time of writing at the University of Technology in Vienna that researches different statistical trend analysis approaches for preservation planning. Thus, we believe that such content profiling output data can be of very high value to DP activities if it can be structured and processed by other software.

% Simulation environments
% cite or refer to Stefan Schindlers thesis and give him credit
% may be say that this was possible with the backend of the tool presented in the next chapter.

\begin{figure}[b]
\begin{center}
\includegraphics[width=3.6in]{figures/contentprofiling/trends_html.png}
\caption{Usage trends in html versions over time in a danish web archive}
\label{fig:trends_html}
\end{center}
\end{figure}

\section{Representative Sets}
\label{sec:representative_sets}
% what is the problem,
% how do we create them/find them
TBD
