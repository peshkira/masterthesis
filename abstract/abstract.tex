%% Based on a TeXnicCenter-Template, which was
%% created by Christoph Brensen
%% and slightly modified by Tino Weinkauf.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ******** Documentclass and Titleformat *********
\documentclass[a4paper,12pt]{scrartcl} %This is a special class provided by the KOMA script, which does a lot of adjustments to adapt the standard LaTeX classes to european habits, change to [a4paper,12pt,twoside] for doublesided layout
\pagestyle{empty}
\renewcommand*{\titlepagestyle}{empty}

\usepackage{psboxit,pstricks}
\makeatletter
\def\thickhrulefill{\leavevmode \leaders \hrule height 1pt\hfill \kern \z@}
\renewcommand{\maketitle}{\begingroup
%  \null\thispagestyle{empty}%
    \let\footnotesize\small
    \let\footnoterule\relax
    \parindent \z@
    \reset@font
    \vskip 1\p@
    \begin{flushleft}
      \LARGE
      \strut\@title
    \end{flushleft}
    \@author \\
    \@date
    \vskip 40\p@
  \endgroup
  \setcounter{footnote}{0}%
}

%########################### Preferences #################################

% ******** vmargin settings *********
\usepackage{vmargin} %This give you full control over the used page arae, it maybe not the idea od Latex to do so, but I wanted to reduce to amount of white space on the page
\setpapersize{A4}
\setmargins{3.2cm}%			%linker Rand, left edge
					 {1.5cm}%     %oberer Rand, top edge
           {14.7cm}%		%Textbreite, text width
           {23.42cm}%   %Texthoehe, text hight
           {14pt}%			%Kopfzeilenh�he, header hight
           {1cm}%   	  %Kopfzeilenabstand, header distance
           {0pt}%				%Fu�zeilenhoehe footer hight
           {2cm}%    	  %Fusszeilenabstand, footer distance         

% ********* Font definiton ************
\usepackage{t1enc} % as usual
\usepackage[latin1]{inputenc} % as usual
\usepackage{times}

% ********* Used packages ************
\usepackage{enumitem}

%################ End Preferences, Begin Document #####################





\title{Content Profiling for Digital Preservation}
\author{Petar Petrov, 0508142}
\date{April 24, 2012}

\begin{document}
\maketitle

\section*{Problem}
Information Technology enables us to organize our digital content into collections of objects in an easy fashion and thus massive volumes of data are produced each day.
However it opens up a huge set of technical and social issues regarding its safety and long-term accessibility \cite{Lorie:2001:LTP:379437.379726}.

Digital Preservation copes with such issues related to hardware and software obsolescence. Among its various
options are tools like emulation, bit-stream and logical preservation. In order to make a meaningful decision
about the potential alternatives or preservation actions that should or should not be executed on a
digital collection, preservation planning is conducted. A preservation plan specifies a concrete action plan for the preservation
of a certain set of objects or a collection and includes potential alternatives and reasons for the decision making; it
consists of the following elements: identification, status and triggers, description of the institutional setting, description of the collection, requirements for preservation, evidence of decision for a preservation strategy, costs, roles and responsibilities and preservation action plan \cite{Becker:2009fk}. 

As collections, which have to be preserved in practice are often huge (in the order of tera- and even petabytes consisting of millions of objects) it is not feasible to create a plan based on experiments over the entire collection. For this reason, inarguably one of the most important parts of preservation planning is the description of the collection or in other words the content profile. In general, the process of content profiling consists of three parts; characterization, aggregation and analysis. Characterization is responsible for the extraction of meta data and the identification of the digital objects, while aggregation offers a compressed view on them. In the last step of analysis, relevant aspects of the content are found and presented for further processing by preservation planning. Thus the process of content profiling shall give a more detailed description about a collection, as a profile consists not only of simple measures such as size, count of objects and format but also provides a deeper view based on any other feature and its distribution, that is of interest for preservation planning.

Another crucial task that can only be done with a solid content profile is the selection of a meaningful and valid representative subset of a collection \cite{Becker:2011:PDT:1998076.1998089, Pan05findingrepresentative}. Only with such a representative collection of sample objects a planner 
has the needed basis to conduct valid experiments and back up her decisions when choosing the best
preservation action alternative. What is more, evidence for the validity and effectiveness of the considered preservation actions can be easily found, based on the representative subset.

In this thesis we observe the existing gap in terms of content profiling and its importance within preservation
planning. Digital content repositories, such as ESciDoc\footnote{https://www.escidoc.org/} and EPrints\footnote{http://www.eprints.org/} are used to store massive volumes of data, however there is no possiblity for large scale data analysis over the content stored and only simple measures such as number of objects and content types are provided. Thus a content profiling capability in such repositories would serve as a valuable input to preservation planning processes. We present a concept for the process of content profiling, a prototype that shall form the basis of an advanced collection profiling step for the preservation planning tool PLATO\footnote{http://ifs.tuwien.ac.at/dp/plato} as presented in \cite{Rauber:2009:dpchallenges}. We provide a comparison of some representative
set algorithms and their implementations as part of the prototype and evaluate them over bigger data collections.

\section*{Expected Result}
A concept for the collection profiling process, an architecture for a software tool and a specification that defines a content profile, which can serve as an input to preservation planning.

A prototype implementation of the tool is to be provided, that is able to cope with the meta data of digital collections and can generate a content profile following this well-defined specification designed in the previous step. The tool will allow users and other tools (developed outside of the scope of this thesis) to do some more comprehensive analysis over the collection properties. We develop and evaluate an algorithm to find a representative subset of a provided collection, based on user specified properties.

At last a use case over a collection of almost 1 million files provided by DigitalCorpora\footnote{digitalcorpora.org} will be conducted. By applying the tool on such a collection we evaluate its capabilities and the effectiveness of the profiling and representative subset finding methods over larger collections.

\section*{Methodical Approach}
The steps to achieve the results mentioned above are the following:
\begin{enumerate}[itemsep=2pt, parsep=2pt]
\item Research the current methods in other research projects and institutional practices regarding content and collection profiling.
\item Analyze the issues and the existing gap between the idea of content profiling and the actual steps done.
\item Design a content profile specification, the architecture of a profiling tool and implement a prototype, that is able to produce output conforming to that specification.
\item Research about applicable algorithms able to find a (representative) subset of a given collection of characterized digital objects and implement such a solution.
\item Conduct a use case study by applying the tool on a larger collection of files and evaluate the results considering scalability and the effectiveness of the representative samples selection process.
\item Draw conclusions based on step 1, 2 and 5
\end{enumerate}

\section*{State of the Art}
Content Profiling is an important part of Preservation Planning and is done to some extent by many software tools. However, the current focus lies in characterization, where many projects such as (Droid\footnote{http://droid.sourceforge.net/}, JHOVE\footnote{http://hul.harvard.edu/jhove/}, JHOVE2\footnote{http://www.jhove2.org}, FITS\footnote{http://code.google.com/p/fits/}, Apache Tika\footnote{http://tika.apache.org/}, etc.) have provided more or less robust tools, able to identify or characterize single objects \cite{Knijff:2011it}. 

Currently a shift is undergoing, where characterization is applied on a larger scale. The aggregation and analysis steps, however, are almost or entirely skipped within many profiling processes and experiments for the preservation actions are conducted on randomly chosen files. None or few tools provide a well-specified way of creating a profile and the ones that do are missing important aspects that are relevant for planning.

Thus the analysis and design of such a content profiling process as well as the design of a profile specification should bring the idea of profiling one step forward and the implementation of a prototype tool would enhance preservation planing effectiveness or at least provide stronger evidence why the chosen actions have to be conducted.

\section*{Software Engineering \& Internet Computing}
Content Profiling is a task that combines both large scale analysis and data aggregation. These represent core tasks of computer science research. Profiling defines a process that is content type independent and applicable to static file system collections of digital data but also to web archives that are updated continuously and thus has to cope with Internet Computing issues as well.

Not only does this thesis focus on a real-life problem in the field of digital preservation, but also provides and evaluates an applicable software solution to it. From the theory of this high-level problem through designing a concrete solution and architecture to the specific implementation it includes parts of many relevant aspects of software engineering. Some of these are the ability to identify (reoccurring) technical problems within a specific domain and apply suitable data structures, algorithms and software design patterns, to cope with the integration of existing software products and APIs, to design robust and clear interfaces yet capable of coping with change, but mainly the competence to understand trade-offs between different options and choose the correct ones under specific circumstances as well as the skill to carry out a software project through all its phases.

\bibliographystyle{plain}
\bibliography{references}

\end{document}



